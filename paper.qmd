---
title: "Ideal Point Estimation With 99% Missing Data"
author: "Bertrand Wilden"
date: "`r Sys.Date()`"
format:
  pdf:
    documentclass: article
    number-sections: true
    geometry: 
      - left=25mm
      - right=25mm
    indent: true
    fontsize: 11pt
    linestretch: 2
    fig-cap-location: top
    include-in-header:
      text:
        \usepackage{amsmath}
        \usepackage{bm}
bibliography: [references.bib, packages.bib]
nocite : |
  @R-targets, @R-here, @R-dplyr
execute: 
  echo: false
  message: false
  warning: false
---

```{r}
pacman::p_load(
  "targets",
  "here",
  "dplyr",
  "tidyr",
  "lubridate",
  "purrr",
  "ggdist",
  "bayesplot",
  "pscl",
  "ggstance",
  "patchwork",
  "ggplot2",
  "MetBrewer",
  "brms"
)

knitr::write_bib(.packages(), "packages.bib")
```

# Introduction

Many variables of interest in the social sciences defy easy measurement. Some latent characteristics, such as ideology, cannot be observed directly and must instead be inferred from the actions taken by political actors. One popular statistical method for estimating latent characteristics from observed actions is known as Item Response Theory (IRT). These models originated in psychometric research in an effort to measure the ability of individual test takers [@rasch1980], but have also been adapted to measure political ideology [@clinton2004]. In general, IRT models can be used to measure latent traits when a set of actors signal their observed preference among two or more choices. A common example of is legislators voting yea or nay on bills in Congress. IRT has been used to measure the political ideology of the US public [@treier2009; @caughey2015], of Supreme Court justices [@martin2002], of Twitter users [@barberá2015], and the alignment of countries in the UN General Assembly [@bailey2017].

While these ideal point models do not strictly require that the entire set of actors in question take visible positions on every policy choice presented before them, in practice this is almost always the case. Most members of the United States Congress vote on most bills, most Supreme Court justices take positions on cases presented before them, and most survey respondents answer all questions in their survey. There may be other contexts, however, where not responding or abstaining are more common. Failing to factor abstentions into models of actors' ideology is valid only if these missing positions are ignorable---in other words, if the actors are abstaining at random rather than due to their ideological characteristics. @rosas2015 show that non-ignorable abstentions can lead to misleading estimates of legislator ideology in legislatures outside the US. In the Israeli Knesset, for example, abstentions are common and likely represent some aspect of legislators' ideologies.

Another context with high rates of missing data are interest groups' signals of support or opposition for particular pieces of legislation. Using Maplight data[^1] on interest group positions, @crosson2020 measure the political ideology of interest groups lobbying in the US Congress. Although the Maplight data has information on 8,494 groups taking one or more positions across 16,436 bills, the probability that any particular group sends a signal on a particular bill is extremely low. *Most* groups fail to signal a position on *most* bills. If interest groups in the US were a legislative assembly, they would have an abstention rate of over 99.99%.

[^1]: Lorenz, Geoffrey M., Alexander C. Furnas, and Jesse M. Crosson. "Large-N Bill Positions Data from MapLight.Org: What Can We Learn from Interest Groups' Publicly Observable Legislative Positions?" *Interest Groups & Advocacy* 9, no. 3 (September 2020): 342--60. <https://doi.org/10.1057/s41309-020-00085-x>.

In this paper I build on the IRT model developed by @rosas2015 to account for the massive amounts of missing data among interest group positions. This model allows each group to have an independent "indifference" parameter which controls the probability that they will signal any position on a particular bill or choose to abstain. The indifference parameter could reflect a number of interest group characteristics. Some interest groups, such as the business-focused Chamber of Commerce, could have issue-areas which touch a wider range of legislation than more narrowly focused groups, such as the National Rifle Association. "Indifference" could also reflect the amount of resources an interest group has---sending signals on legislation requires time and effort. Ultimately, however, I am indifferent to the source of interest group indifference in this project.

My primary goal is extending the analysis done by @rosas2015, and showing how common IRT techniques used in political science dramatically fail in contexts where a vast majority of signals are missing. I also contribute to overall use of IRT models in political science by incorporating the latest advances in the field of Bayesian computational methods into the analysis. Finally, I replicate the ideal point estimation from @crosson2020 and show how my model yields a diverging view of political polarization among interest group lobbying.

# Ideal Point Models

Ideal point models in political science are types of measurement models. These methods use observed actions to inform us about the latent traits of various political actors. Using interest groups as an example, let's imagine that each group, $i$ has an political ideal point, $\theta_i$ which lies on a single left-right, or liberal-conservative, scale. The probability that a given group signals its support or opposition to a particular piece of legislation, $j$ is determined by the distance between the group's ideal point and the ideological content of the bill, $\gamma_j$. We also assume that each bill has a component, $\xi_j$ which controls the probability, independent of political ideology, that it receives support from an interest group. Putting these parameters together, along with an assumption that interest groups have some Normal(0, 1) idiosyncratic error in their signalling behavior (which we encode using the standard normal cumulative density function, $\Phi$), gives us the following model which can be estimated from observed data:

$$
\begin{aligned}
  \text{Pr}(y_{ij} &= \text{Support}) = \Phi(\gamma_j\theta_i + \beta_j)
\end{aligned}
$$ {#eq-irt}

```{r}
#| label: fig-irt
#| fig-cap: "Binary IRT Model"
#| fig-height: 4
tau_plus <- .7
tau_minus <- -2
tau_mid <- (tau_plus + tau_minus) / 2

plot_labs <- tibble(x = c(tau_mid, 0),
                    y = c(-.02, .03),
                    label = c("0", "gamma[j] * theta[i] + xi[j]"))
tibble(z = seq(from = -3.75, to = 3.75, length.out = 1e3)) %>% 
  mutate(d = dnorm(x = z, mean = 0, sd = 1)) %>% 
  ggplot(aes(x = z, y = d)) +
  geom_line(color = "black") +
  geom_area(aes(fill = z <= tau_mid), alpha = 1/4) +
  geom_segment(x = tau_mid, xend = tau_mid, y = 0, yend = dnorm(tau_mid), linetype = 3)+
  geom_segment(x = tau_mid, xend = tau_mid, y = 0, yend = .01) +
  geom_segment(x = 0, xend = 0, y = 0, yend = .01) +
  annotate("text", x = plot_labs$x, y = plot_labs$y, label = parse(text = plot_labs$label),
           size = 4, family = "serif") +
  annotate("text", x = 2.7, y = dnorm(1), 
           label = parse(text = "Pr(y[ij] == Support)"),
           size = 4, family = "serif") +
  geom_curve(aes(x = 2.7, y = dnorm(1.1),
             xend = 1, yend = .1),
             arrow = arrow(length = unit(.02, "npc")), curvature = -.25) +
  annotate("text", x = -2.7, y = dnorm(-1.5), 
           label = parse(text = "Pr(y[ij] == Oppose)"),
           size = 4, family = "serif") +
  geom_curve(aes(x = -2.7, y = dnorm(-1.6),
             xend = -1.6, yend = .05),
             arrow = arrow(length = unit(.02, "npc")), curvature = .25) +
  geom_hline(yintercept = 0) +
  scale_x_continuous(NULL, breaks = NULL, name = parse(text = "y[ij]")) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(-3.25, 3.25)) +
  scale_fill_manual(values = met.brewer("Isfahan1", n = 2),
                    breaks = NULL) +
  theme_ggdist() +
  theme(axis.line.y.left = element_blank(),
        panel.background = element_rect(fill = "transparent",colour = NA),
        plot.background = element_rect(fill = "transparent",colour = NA),
        text = element_text(size = 14, family = "serif"))
```

The binary IRT model from @eq-irt is shown graphically in @fig-irt. The shaded density regions are proportional the relative probability of a particular interest group either supporting or opposing a particular bill ($y_{ij}$ in the data). Larger values of $\gamma_j\theta_i + \xi_j$ lead to higher probabilities that $y_{ij} = \text{Support}$, and lower values lead to higher probabilities that $y_{ij} = \text{Oppose}$. If $\gamma_j\theta_i + \xi_j = 0$, the interest group is 50-50 on whether to support or oppose the bill. This follows from the mathematical evaluation of the Normal(0, 1) CDF at zero: $\Phi(0) = 0.5$.

@eq-irt is equivalent to the 2-parameter IRT model used in psychometric research [@fox2010]. As it stands, however, @eq-irt is not identified. This means that, for given data, $y_{ij}$ there is not a unique set of parameter values for $\theta_i$, $\gamma_j$, and $\xi_j$. There for three reasons for this. First, there is no unique location for the latent scale because adding a constant to the $\gamma_j\theta_i$ term can be offset by subtracting the same constant from $\beta_j$. In other words, while it may be natural to think of zero as the center of a political ideology scale, nothing in @eq-irt defines the center of the latent variable. Second, the scale can be arbitrarily stretched or compressed by multiplying and dividing the terms by the same constant. And third, the polarity of the scale is not uniquely identified. There is no information in @eq-irt to tell use whether larger values of $\theta_i$ correspond to more liberal ideal points or to more conservative ideal points.

While @eq-irt assumes that the response data are binary (support or oppose) and uses the probit link function, $\Phi$ to transform the nonlinear predictor into a probability, IRT models can support a wide range of other outcome types [@bürkner2020]. Public opinion surveys often ask people to rank their support or opposition to some policy on a five or seven point scale. In these situations either an ordinal or multinomial link function is more appropriate rather than collapsing responses to a binary support/oppose [@hill2015]. Likelihoods such as the poisson distribution can also be used if the observed outcomes are a count of positions taken [@slapin2008]. As I demonstrate below, the choice of response function in IRT ideal point models can have a dramatic effect on inferences.

# Abstention Ideal Point Model

Recall that the Maplight data reports interest group positions (support/oppose) on bills brought before the US Congress. While it may appear that these observed positions are binary, interest groups have a third option available to them when a piece of legislation is introduced: to abstain from taking any position at all. In fact, this is by far the most common response interest groups choose. For any given bill, over 99.99% of interest groups will not send a signal of either support or opposition. Most interest groups focus on narrow policy areas, and sending a signal of support an opposition is costly, so this high abstention rate should be expected. Ideal point models, however, assume that *all* pieces of legislation contain some amount of ideological content along the uni-dimensional left-right scale. Therefore we should consider interest group abstentions as something like a middle-ground position that lies between signals of outright support and outright opposition.

@rosas2015 develop an IRT ideal point model which explicitly accounts for abstentions in legislatures. Every actor is given an additional parameter $\tau_i$ which controls its level of indifference. The larger the absolute value of $\tau_i$, the less likely the actor will signal support or opposition on a given bill. @eq-irt-tau and @fig-irt-abs show the expanded model:

$$
\begin{aligned}
   \text{Pr}(y_{ij} = \text{Support}) &= \Phi(\gamma_j\theta_i + \beta_j - \tau_i) \\[-10pt]
   \text{Pr}(y_{ij} = \text{Abstain}) &= \Phi(\tau_i - (\gamma_j\theta_i + \beta_j)) - \Phi(-\tau_i - (\gamma_j\theta_i + \beta_j)) \\[-10pt]
   \text{Pr}(y_{ij} = \text{Oppose}) &= 1 - \Phi(\gamma_j\theta_i + \beta_j + \tau_i) 
\end{aligned}
$$ {#eq-irt-tau}

```{r}
#| label: fig-irt-abs
#| fig-cap: "Abstention IRT Model"
#| fig-height: 4
plot_labs <- tibble(x = c(tau_minus, tau_mid, 0, tau_plus),
                    y = c(-.02, -.02, .03, -.02),
                    label = c("-tau[i]", "0", "gamma[j] * theta[i] + xi[j]", "tau[i]"))
tibble(z = seq(from = -3.75, to = 3.75, length.out = 1e3)) %>% 
  mutate(d = dnorm(x = z, mean = 0, sd = 1)) %>% 
  ggplot(aes(x = z, y = d)) +
  geom_line(color = "black") +
  geom_area(aes(x = z, y = ifelse(z <= tau_minus, d, 0)), fill = "#178f92", alpha = 1/4) +
  geom_area(aes(x = z, y = ifelse(z > tau_minus & z <= tau_plus, d, 0)), fill = "transparent", alpha = 1/4) +
  geom_area(aes(x = z, y = ifelse(z > tau_plus, d, 0)), fill = "#845d29", alpha = 1/4) +
  geom_segment(x = tau_plus, xend = tau_plus, y = 0, yend = dnorm(tau_plus), linetype = 3) +
  geom_segment(x = tau_minus, xend = tau_minus, y = 0, yend = dnorm(tau_minus), 
               linetype = 3) +
  geom_segment(x = tau_plus, xend = tau_plus, y = 0, yend = .01) +
  geom_segment(x = tau_minus, xend = tau_minus, y = 0, yend = .01) +
  geom_segment(x = tau_mid, xend = tau_mid, y = 0, yend = .01) +
  geom_segment(x = 0, xend = 0, y = 0, yend = .01) +
  annotate("text", x = plot_labs$x, y = plot_labs$y, label = parse(text = plot_labs$label),
           size = 4, family = "serif") +
  annotate("text", x = tau_plus + 2, y = dnorm(tau_plus + .5), 
           label = parse(text = "Pr(y[ij] == Support)"),
           size = 4, family = "serif") +
  geom_curve(aes(x = tau_plus + 1.9, y = dnorm(tau_plus + .6),
             xend = tau_plus + 1.1, yend = dnorm(tau_plus + 1.5)),
             arrow = arrow(length = unit(.02, "npc")), curvature = -.25) +
  annotate("text", x = tau_minus - .7, y = dnorm(tau_minus + .5), 
           label = parse(text = "Pr(y[ij] == Oppose)"),
           size = 4, family = "serif") +
  geom_curve(aes(x = tau_minus - .7, y = dnorm(tau_minus + .4),
             xend = tau_minus - .5, yend = dnorm(tau_minus - .8)),
             arrow = arrow(length = unit(.02, "npc")), curvature = .25) +
  annotate("text", x = tau_mid - 1, y = dnorm(tau_mid - .2), 
           label = parse(text = "Pr(y[ij] == Abstain)"),
           size = 4, family = "serif") +
  geom_curve(aes(x = tau_mid - 1, y = dnorm(tau_mid - .3),
             xend = tau_mid - .4, yend = dnorm(tau_mid) / 2),
             arrow = arrow(length = unit(.02, "npc")), curvature = .25) +
  geom_hline(yintercept = 0) +
  scale_x_continuous(NULL, breaks = NULL, name = parse(text = "y[ij]")) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(-3.25, 3.25)) +
  theme_ggdist() +
  theme(axis.line.y.left = element_blank(),
        panel.background = element_rect(fill = "transparent",colour = NA),
        plot.background = element_rect(fill = "transparent",colour = NA),
        text = element_text(size = 14, family = "serif"))
```

We can estimate the group and bill parameters in @eq-irt-tau using a Bayesian ordered probit model with group-specific thresholds. A fully-specified Bayesian model requires that we place priors on all parameters. This has the added benefit in the context of IRT models of resolving the location and scale identification issues described earlier. Priors work to constrain the plausible space of parameter values, which means we can use them to enforce a center on the ideal point distribution as well as set a reasonable scale to the values. In my model I assign the following prior distributions to the main parameters:

$$
\begin{aligned}
  \tau_i &\sim \text{Normal}(0, 4) \\[-10pt]
  \theta_i &\sim \text{Normal}(0, 3) \\[-10pt]
  \gamma_j &\sim \text{Normal}(0, 3) \\[-10pt]
  \beta_j &\sim \text{Normal}(0, 2)
\end{aligned}
$$

In order to identify the polarity of the ideal point scale I include covariates to help predict the $\theta_i$ and $\gamma_j$ parameters.[^2] A binary indicator for whether the interest group is classified as representing business interests, along with a binary indicator for whether the bill was authored by a Republican member of Congress, are added to the model. I constrain each of these coefficients to be positive by using a LogNormal(-1, 1) prior distribution. The center of this distribution is $exp(-1) = 0.368$ which is reasonably conservative on the probit scale. But the right-skewed nature of the LogNormal distribution allows these coefficients to be much larger if required by the data. Because $\theta_i$ and $\gamma_j$ now modeled with a linear predictor, they are given intercepts $\alpha_\theta$ and $\alpha_\gamma$.

[^2]: The parameter $\xi_j$ does not get any covariates because it represents the non-ideological component of the bill, and therefore is not relevant to the polarity identification issue.

$$
\begin{aligned}
  \theta_i &\sim \text{Normal}(\alpha_\theta + \delta_1\text{Business}_i, 3) \\[-10pt]
  \gamma_j &\sim \text{Normal}(\alpha_\gamma + \delta_2\text{Republican}_j, 3) \\[-10pt]
  \xi_j &\sim \text{Normal}(0, 2) \\[-10pt]
  \delta_1, \delta_2 &\sim \text{LogNormal}(-1, 1) \\[-10pt]
  \alpha_\theta, \alpha_\gamma &\sim \text{Normal}(0, 2)
\end{aligned}
$$

The method of including informative covariates into ideal point models is not widely practiced in political science. Instead, in order to identify the polarity of the scale researchers typically fix the ideal points of two actors to constants [@bafumi2005]. An example from the US Congress would be to fix liberal Bernie Sanders's ideal point to -2 and conservative Ted Cruz's to 2. This method has a few drawbacks. First, the results become sensitive to the modeler's *a priori* expectations for where certain actors are located on the latent scale. This might not be such a problem in contexts which have been heavily studied like US Congress members, but there are fewer theoretical expectations for where the ideal points of specific interest groups are located. Second, it is not necessarily possible to fix actors to specific locations when using hierarchical modeling techniques.

Hierarchical Bayesian modeling is another under-utilized technique among IRT ideal point models in political science. Rather than estimating each actor and bill parameter in a vacuum, hierarchical modeling (also referred to as multilevel modeling) allows the groups to partially-pool information from the population of actors and bills. This is done by modeling each parameter's prior as a function of the group mean, which itself is given a prior. Extending out the model with hierarchical priors we get:

$$
\begin{aligned}
  \tau_i &\sim \text{Normal}(0, 4) \\[-10pt]
  \theta_i &\sim \text{Normal}(\bar{\theta}, \sigma_\theta) \\[-10pt]
  \bar{\theta} &= \alpha_\theta + \delta_1\text{Business}_i \\[-10pt]
  \gamma_j &\sim \text{Normal}(\bar{\gamma}, \sigma_\gamma) \\[-10pt]
  \bar{\gamma} &= \alpha_\gamma + \delta_2\text{Republican}_j \\[-10pt]
  \xi_j &\sim \text{Normal}(\bar{\xi}, \sigma_\xi) \\[-10pt]
  \delta_1, \delta_2 &\sim \text{LogNormal}(-1, 1) \\[-10pt]
  \sigma_\theta &= 1 \\[-10pt]
  \sigma_\gamma, \sigma_\xi &\sim \text{HalfCauchy}(0, 2)
\end{aligned}
$$

The $\sigma_\theta$ parameter is fixed at 1 in order to help identify the scale of the ideal points [@bürkner2020; @rosas2015], but the rest of the variance parameters are allowed to vary. Hierarchical modeling is very powerful in contexts like the MapLight data. Many interest groups only signal support or opposition a few times during the time period covered, so it is important that we use the population distribution of interest group ideal points to help inform us about these rare cases. Hierarchical models are also much better at predicting out of sample compared to ordinary "memory-less models" [@gelman2007; @mcelreath2020]. This should give us more confidence that the ideal points produced by the model are a more accurate reflection of the interest groups' true latent ideology.

$$
\begin{aligned}
  y_{ij} &\sim \text{Ordered.Categorical}(\mathbf{p}) \\[-10pt]
  p_2 &= \Phi(\gamma_j\theta_i + \beta_j - \tau_i) \\[-10pt]
  p_1 &= \Phi(\tau_i - (\gamma_j\theta_i + \beta_j)) - \Phi(\tau_i -   (\gamma_j\theta_i + \beta_j)) \\[-10pt]
  \tau_i &\sim \text{Normal}(0, 4) \\[-10pt]
  \theta_i &\sim \text{Normal}(\bar{\theta}, \sigma_\theta) \\[-10pt]
  \bar{\theta} &= \alpha_\theta + \delta_1\text{Business}_i \\[-10pt]
  \gamma_j &\sim \text{Normal}(\bar{\gamma}, \sigma_\gamma) \\[-10pt]
  \bar{\gamma} &= \alpha_\gamma + \delta_2\text{Republican}_j \\[-10pt]
  \xi_j &\sim \text{Normal}(\bar{\xi}, \sigma_\xi) \\[-10pt]
  \delta_1, \delta_2 &\sim \text{LogNormal}(-1, 1) \\[-10pt]
  \sigma_\theta &= 1 \\[-10pt]
  \sigma_\gamma, \sigma_\xi &\sim \text{HalfCauchy}(0, 2)
\end{aligned}
$$ {#eq-full}

Putting all the pieces of the model together gives us @eq-full. This is an ordered probit model with three outcomes: support, abstain, and oppose. The cutpoint $p_1$ reflects the probability of an interest group supporting a particular bill, and the cutpoint $p_2$ reflects the probability of abstaining. The probability of opposing a particular bill is implicit in the remaining probability density, or $1 - \Phi(\gamma_j\theta_i + \beta_j + \tau_i)$. I fit this model using the probabilistic programming language Stan [@carpenter2017]. Stan's Hamiltonian Monte Carlo sampler does a better job of exploring the high dimensional parameter space in a model like @eq-full than older Markov Chain Monte Carlo algorithms [@betancourt2018]. It also provides many diagnostic tools which alert users of possible problems with their models. This is invaluable for IRT models for which proper identification is a big concern.

The ordered probit model which accounts for interest group abstentions in @eq-full (hereafter referred to as the "Abstention model") stands in contrast with the ideal point model used by @crosson2020. The authors use the binary ideal point model developed by @clinton2004 via the R package **pscl**. The default **pscl** ideal point model uses @eq-irt with the following priors:

$$
\begin{aligned}
  \theta_i &\sim \text{Normal}(0, 1) \\[-10pt]
  \gamma_j &\sim \text{Normal}(0, 25) \\[-10pt]
  \beta_j &\sim \text{Normal}(0, 25)
\end{aligned}
$$ {#eq-pscl}

This method does not model abstentions explicitly, nor does it take advantage of interest group and bill covariates or use hierarchical priors. These drawbacks are not always apparent, and the **pscl** model does an excellent job recovering accurate ideal points in many contexts in which it has been used. But as I show in the following section, it is ill-suited for estimating ideal points when the data generating process produces massively abstention-skewed observations. And the MCMC method used by **pscl** is not equipped with the same diagnostic tools as Stan's HMC sampler to alert users that these problems may be occurring.

# Simulation Study

```{r}
tar_load(sim_data_ord)
tar_load(sim_ord_comparison_plots)

calc_missing <- function(n) {
  pct_missing <- scales::percent(mean(sim_data_ord[[n]]$ij_all$position == 2), .1)
  return(pct_missing)
}
```

To test the accuracy of the Abstention model from @eq-full and the **pscl** model, I simulate five data sets with 100 interest groups and 300 bills. Each interest group is given an ideal point drawn from a Normal(0, 1) distribution and an indifference parameter drawn from a $\text{Normal}_+(\mu, 0.5)$ distribution with a mean,$\mu$ varying across the five simulated data sets. Each interest group either supports, abstains, or opposes a given bill following the formula in @eq-irt-tau. Increasing $\mu$ from 0 to 4 by increments of 1 produced the following abstention rates in five data sets: `r paste(purrr::map(1:5, calc_missing), sep = ", ")`. The Abstention model is fit on each of these full simulated data sets, whereas the **pscl** model is fit on a version of each data set with the abstention observations dropped.[^3]

[^3]: Simulated groups which never signal support or opposition are removed from both versions of the data so that each model is working with the same set of groups.

```{r, fig.cap=paste("Simulation Results with", calc_missing(1), "Missing Data")}
#| fig-height: 4
#| label: fig-sim-1
sim_ord_comparison_plots[[1]]
```

```{r, fig.cap=paste("Simulation Results with", calc_missing(2), "Missing Data")}
#| fig-height: 4
#| label: fig-sim-2
sim_ord_comparison_plots[[2]]
```

```{r, fig.cap=paste("Simulation Results with", calc_missing(3), "Missing Data")}
#| fig-height: 4
#| label: fig-sim-3
sim_ord_comparison_plots[[3]]
```

```{r, fig.cap=paste("Simulation Results with", calc_missing(4), "Missing Data")}
#| fig-height: 4
#| label: fig-sim-4
sim_ord_comparison_plots[[4]]
```

```{r, fig.cap=paste("Simulation Results with", calc_missing(5), "Missing Data")}
#| fig-height: 4
#| label: fig-sim-5
sim_ord_comparison_plots[[5]]
```

@fig-sim-1 through @fig-sim-5 display the results of these simulation tests. In each plot the true interest group ideal points, $\theta_i$ are displayed along the x-axis and the ideal points estimated from each model, $\hat{\theta}_i$ are displayed along the y-axis. Each model's performance can be judged by how closely the estimated ideal points track with the true ideal points along the solid diagonal identity line. The 89% posterior intervals around the median ideal points also provides a sense for how precise the estimates are.

Simulations with abstention rates below 75% (@fig-sim-1, @fig-sim-2, and @fig-sim-3) show both models performing well. But once the abstention rate starts to exceed 90% in @fig-sim-4, the **pscl** model begins to deteriorate. We see a decoupling of the estimates from their true values throughout the range of ideal points. Things really fall apart for the **pscl** model in @fig-sim-5 with `r calc_missing(5)` abstentions. The estimated ideal points are all over the place and fail to discernibly correlate with the true ideal points. As we increase the percentage of abstentions, the Abstention model produces more uncertain estimates as well. But overall the Abstention model is roughly recovering the true ideal point values in all five simulations.

Although the simulation results show some evidence that the Abstention model is better equipped to handle missing data compared to the **pscl** model, the former has one major drawback. The Abstention model takes an extremely long time to fit compared to **pscl**. The combination of modeling all group and bill parameters hierarchically and giving each group its own independent thresholds on the ordered probit scale increase computational demands drastically. Each of the simulations in @fig-sim-1 through @fig-sim-5 above took roughly two hours to run on a 2019 MacBook Pro. Whereas fitting the **pscl** models took less than five seconds each. So for now the Abstention model is only feasible for small-scale data sets.

# Replication of *Polarized Pluralism: Organizational Preferences and Biases in the American Pressure System*

```{r}
tar_load(cfl_exp_data)

cfl_missing = scales::percent(mean(cfl_exp_data[[1]]$ij_all$position == 2), .1)
```

In *Polarized Pluralism: Organizational Preferences and Biases in the American Pressure System* (2020) Crosson, Furnas, and Lorenz use the MapLight data on interest groups' lobbying behavior in the US Congress to construct ideal points for interest groups. Rather than exhibiting a strong conservative bias as was previously theorized [@schattschneider1960], the authors find that the ideological distribution of interest groups is distinctly bi-modal and roughly resembles the ideological make up of members of Congress. As mentioned previously, the authors use the **pscl** model from @clinton2004 to estimate these interest group ideal points (@eq-pscl).

Because of the expensive computation required by the Abstention model, I was only able to use a small portion of their data to replicate their results. Looking only at the 113th Congress (2013 to 2015), I selected the 200 interest groups with the highest amount of support/opposition signals. I then selected the 600 bills which received the highest amount of support/opposition signals and expanded the data set such that if a group did not signal support or opposition on a particular bill its observed action was explicitly coded as an abstention.[^4] The overall abstention rate in this reduced data is `r cfl_missing`.

[^4]: A handful of the 200 groups never signaled on any of the 600 bills and were thus dropped from the data. Also bills that received less than five support/oppose signals were dropped from the data. Unlike Crosson et al., I did *not* remove bills that received unanimous signals of support or opposition. While such bills might provide no evidence of a group's ideal point using the **pscl** model, the fact that a group choose to signal rather than to abstain provides valuable information in the Abstention model.

```{r}
#| label: fig-cfl-density
#| fig-cap: "Federal Interest Group Ideal Point Distributions"
#| fig-height: 4
tar_load(cfl_density_plot)
cfl_density_plot
```

@fig-cfl-density shows the distribution of median $\theta_i$ estimates across the two models. The **pscl** model produces a distinctly bi-modal distribution of interest group ideology, which, despite the restricted sample, replicates the main findings from @crosson2020. The Abstention model, on the other hand, shows many more interest groups falling in the middle of the ideological spectrum. There is still some evidence of bi-modality, but the peaks are far less pronounced. As debates about the causes and extent of US political polarization continue, these results show how important good measurement is. The Abstention model, which better reflects the underlying data generating process, provides us a new understanding of the ideological landscape of federal interest groups.

```{r}
#| label: fig-cfl-comp
#| fig-cap: "Federal Interest Group Ideal Point Comparison"
#| fig-height: 7
tar_load(cfl_comparison_plot)
cfl_comparison_plot[[1]]
```

@fig-cfl-comp displays another comparison of interest group ideology model estimates. Each individual interest group's median $\theta_i$ from both models is shown along with the 89% posterior intervals. As in @fig-cfl-density, we can see the clumping of **pscl** estimates in a liberal cluster and a conservative cluster. We also see that the Abstention model's ideologically centrist results are not an artifact of hierarchical partial pooling. Rather than all $\theta_i$'s being pulled to the center, the Abstention model produces estimates which are sometimes further liberal or further conservative than the **pscl** estimates.


# Conclusion

\newpage

# References

::: {#refs}
:::


